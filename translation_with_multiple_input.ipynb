{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f8a2538c1b822e",
   "metadata": {},
   "source": [
    "# Translation with **Multiple** Input Images\n",
    "\n",
    "This notebook calculates the metrics of FID and $L_1$ distance for CollaGAN in the scenarios of it receiving 3, 2 and 1 images as input.\n",
    "\n",
    "It also compares the replacer strategy (of dropout and forward) and the amount of input dropout to apply during training."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6fbde96ae6dfc27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:47:05.579356Z",
     "start_time": "2025-02-12T20:47:04.939335Z"
    }
   },
   "source": [
    "%reset -f\n",
    "\n",
    "import itertools\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "d84c5c9b992fc87c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:47:10.794890Z",
     "start_time": "2025-02-12T20:47:05.599009Z"
    }
   },
   "source": [
    "import logging_utils\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]\n",
    "    )\n",
    "\n",
    "\n",
    "from ModelProxy import CollaGANModelProxy, RemicModelProxy\n",
    "\n",
    "logging_utils.configure(logging.INFO)\n",
    "\n",
    "should_post_process = False\n",
    "\n",
    "model_loaders = {\n",
    "    \"CollaGAN\": lambda: CollaGANModelProxy(\"models/collagan\", post_process=should_post_process),\n",
    "    # \"ReMIC\": lambda: RemicModelProxy(\"models/remic\", post_process=should_post_process),\n",
    "\n",
    "    # Used for CollaGAN's model selection regarding input dropout, and the ablation study:\n",
    "    #\n",
    "    # 'IDNone,Forward': lambda: CollaGANModelProxy('models/selection/collagan/replacer-forward-240k/input-dropout-no', post_process=should_post_process),\n",
    "    # 'IDOrig,Forward': lambda: CollaGANModelProxy('models/collagan/replacer-forward-240k/input-dropout-original', post_process=should_post_process),\n",
    "    # 'IDCurr,Forward': lambda: CollaGANModelProxy('models/collagan/replacer-forward-240k/input-dropout-curriculum', post_process=should_post_process),\n",
    "    # 'IDCons,Forward': lambda: CollaGANModelProxy(\n",
    "    #     'models/model-selection/collagan/replacer-forward-240k/input-dropout-conservative', post_process=should_post_process),\n",
    "    # 'IDOrig,Dropout': lambda: CollaGANModelProxy('models/collagan/replacer-dropout-240k/input-dropout-original', post_process=should_post_process),\n",
    "    # 'Capacity1': lambda: CollaGANModelProxy('models/collagan/replacer-dropout-240k/input-dropout-original-capacity1', post_process=should_post_process),\n",
    "}\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 17:47:05.904746: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739393225.926761 2251634 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739393225.934348 2251634 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-12 17:47:05.958060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flavioro/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/home/flavioro/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/home/flavioro/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl8str_util8EndsWithESt17basic_string_viewIcSt11char_traitsIcEES4_']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/home/flavioro/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/home/flavioro/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/home/flavioro/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZN3tsl8str_util9LowercaseB5cxx11ESt17basic_string_viewIcSt11char_traitsIcEE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "I0000 00:00:1739393230.455091 2251634 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4096 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "7a197eb13469f35",
   "metadata": {},
   "source": [
    "## Loads the Larger Dataset\n",
    "\n",
    "These tests use the Larger dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce1e3b99678db8f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:47:11.710330Z",
     "start_time": "2025-02-12T20:47:11.028162Z"
    }
   },
   "source": [
    "from dataset_utils import DatasetLoader\n",
    "from image_utils import show_single_image\n",
    "\n",
    "\n",
    "dataset_loader = DatasetLoader(\"all\", \"test\", limit=None)\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "1b4b1d264ddaf06a",
   "metadata": {},
   "source": [
    "### Sanity Checks the Dataset\n",
    "\n",
    "Uncomment the last line below to see a sample image from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "929d74ac8d0eb6a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:47:11.735440Z",
     "start_time": "2025-02-12T20:47:11.729489Z"
    }
   },
   "source": [
    "def show_dataset_image():\n",
    "    back, left, front, right = next(iter(dataset_loader.dataset))\n",
    "    show_single_image(front)\n",
    "    \n",
    "# show_dataset_image()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "d6cfb2caeac86713",
   "metadata": {},
   "source": [
    "## Sanity Checking the Models\n",
    "\n",
    "Uncomment the last line to see some images generated by each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e9823fcfc4b8a0fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:47:27.801715Z",
     "start_time": "2025-02-12T20:47:11.785084Z"
    }
   },
   "source": [
    "from image_utils import show_multiple_input_model_comparison\n",
    "import tensorflow as tf\n",
    "\n",
    "def generate_batch_with_each_model(number_of_4batches=1):\n",
    "    # loads a batch of images\n",
    "    batch = next(iter(dataset_loader.dataset.shuffle(600).batch(number_of_4batches*4).take(1)))\n",
    "    batch_transpose = tf.transpose(batch, [1, 0, 2, 3, 4])\n",
    "    \n",
    "    genned_images = []    \n",
    "    target_indices = (tf.repeat(tf.range(0, 4), [number_of_4batches] * 4))\n",
    "    target_images = tf.gather(batch_transpose, target_indices, batch_dims=1)\n",
    "    dropped_mask = tf.one_hot(target_indices, 4, dtype=tf.float32, off_value=1.0, on_value=0.0)\n",
    "    source_images = batch_transpose * dropped_mask[..., tf.newaxis, tf.newaxis, tf.newaxis]\n",
    "    model_names = list(model_loaders.keys())\n",
    "    for _, loader in model_loaders.items():\n",
    "        # loads the model into memory\n",
    "        generator = loader()    \n",
    "        # generates the images\n",
    "        fake_images = generator.generate_from_multiple(target_indices, source_images)\n",
    "        # shows the images\n",
    "        genned_images += [fake_images]\n",
    "        # frees the memory used by the model\n",
    "        del generator\n",
    "\n",
    "    show_multiple_input_model_comparison(source_images, target_images, genned_images, f\"Each side as target\", model_names)\n",
    "\n",
    "generate_batch_with_each_model(4)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 17:47:20 INFO     Fingerprint not found. Saved model loading will continue.\n",
      "2025-02-12 17:47:20 INFO     path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.model.signatures['serving_default']: ConcreteFunction Input Parameters:\n",
      "  source_images (KEYWORD_ONLY): TensorSpec(shape=(None, 4, 64, 64, 4), dtype=tf.float32, name='source_images')\n",
      "  target_domain (KEYWORD_ONLY): TensorSpec(shape=(None, 1), dtype=tf.float32, name='target_domain')\n",
      "Output Type:\n",
      "  Dict[['conv2d_159', TensorSpec(shape=(None, 64, 64, 4), dtype=tf.float32, name='conv2d_159')]]\n",
      "Captures:\n",
      "  139911854647888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341382672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854648080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341382864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854648272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341383056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854648464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341383248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341381328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341381136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341381712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341381520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341382096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341381904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341382480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341382288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854647120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341380368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854647312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341380560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854647504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341380752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854647696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341380944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341379024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341378832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341379408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341379216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341379792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341379600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341380176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341379984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341378064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341378256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341378448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341378640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854646352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341377296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854646544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341377488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854646736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341377680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854646928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341377872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341375952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341375760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341376336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341376144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341376720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341376528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341377104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341376912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854645584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854996944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854645776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854997136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854645968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854997328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854646160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911341375568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854995600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854995408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854995984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854995792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854996368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854996176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854996752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854996560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854994640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854994832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854995024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854995216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854644816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854993872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854645008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854994064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854645200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854994256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854645392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854994448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854992528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854992336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854992912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854992720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854993296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854993104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854993680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854993488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854644048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854991568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854644240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854991760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854644432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854991952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854644624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854992144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854990224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854990032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854990608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854990416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854990992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854990800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854991376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854991184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854989264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854989456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854989648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854989840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854643280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854988496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854643472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854988688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854643664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854988880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854643856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854989072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854987152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854986960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854987536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854987344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854987920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854987728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854988304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854988112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854642512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854986192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854642704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854986384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854642896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854986576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854643088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854986768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854984848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854984656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854985232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854985040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854985616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854985424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854986000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854985808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854984464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854984272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854984080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854983888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854642320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854983696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854983504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854983312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854642128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854983120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854982736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854982928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854982544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854982352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854641936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854982160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854981968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854981776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854641744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854981584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854981392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854981200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854653072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854653264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854641552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854652880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854652688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854652496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854640976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854652304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854652112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854651728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854651536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854641360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854651344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854651152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854650960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854641168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854650768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854650576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854650384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854650192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854650000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854640784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854649808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854649616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854649424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854640592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854649232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854649040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854648848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  139911854648656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-12 17:47:21.284073: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: INVALID_ARGUMENT: axis = 1 not in [-1, 1)\n",
      "\t [[{{function_node __inference__wrapped_model_11426}}{{node CollaGANAffluentGenerator/tf.unstack/unstack}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling MultiInputTFSMLayer.call().\n\n\u001B[1mGraph execution error:\n\nDetected at node CollaGANAffluentGenerator/tf.unstack/unstack defined at (most recent call last):\n<stack traces unavailable>\naxis = 1 not in [-1, 1)\n\t [[{{node CollaGANAffluentGenerator/tf.unstack/unstack}}]] [Op:__inference_signature_wrapper_11625]\u001B[0m\n\nArguments received by MultiInputTFSMLayer.call():\n  • inputs=['tf.Tensor(shape=(16, 4, 64, 64, 4), dtype=float32)', 'tf.Tensor(shape=(16,), dtype=float32)']",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidArgumentError\u001B[0m                      Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 27\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;28;01mdel\u001B[39;00m generator\n\u001B[1;32m     25\u001B[0m     show_multiple_input_model_comparison(source_images, target_images, genned_images, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEach side as target\u001B[39m\u001B[38;5;124m\"\u001B[39m, model_names)\n\u001B[0;32m---> 27\u001B[0m \u001B[43mgenerate_batch_with_each_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[5], line 19\u001B[0m, in \u001B[0;36mgenerate_batch_with_each_model\u001B[0;34m(number_of_4batches)\u001B[0m\n\u001B[1;32m     17\u001B[0m generator \u001B[38;5;241m=\u001B[39m loader()    \n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# generates the images\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m fake_images \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_from_multiple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource_images\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# shows the images\u001B[39;00m\n\u001B[1;32m     21\u001B[0m genned_images \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [fake_images]\n",
      "File \u001B[0;32m/mnt/d/Projetos 2/dissertation-notebooks/ModelProxy.py:329\u001B[0m, in \u001B[0;36mCollaGANModelProxy.generate_from_multiple\u001B[0;34m(self, target_domain, batch_transpose)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_from_multiple\u001B[39m(\u001B[38;5;28mself\u001B[39m, target_domain, batch_transpose):\n\u001B[1;32m    328\u001B[0m     target_domain \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mcast(target_domain, tf\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m--> 329\u001B[0m     genned_image \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_transpose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_domain\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_process:\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;66;03m# batch_transpose has shape (b, d, h, w, c)\u001B[39;00m\n\u001B[1;32m    332\u001B[0m         batch_shape \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mshape(batch_transpose)\n",
      "File \u001B[0;32m~/.virtualenvs/dissertation-notebooks/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/mnt/d/Projetos 2/dissertation-notebooks/ModelProxy.py:21\u001B[0m, in \u001B[0;36mMultiInputTFSMLayer.call\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     19\u001B[0m inputs_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_names, inputs))\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself.model.signatures[\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mserving_default\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m]:\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39msignatures[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mserving_default\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 21\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignatures\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mserving_default\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "\u001B[0;31mInvalidArgumentError\u001B[0m: Exception encountered when calling MultiInputTFSMLayer.call().\n\n\u001B[1mGraph execution error:\n\nDetected at node CollaGANAffluentGenerator/tf.unstack/unstack defined at (most recent call last):\n<stack traces unavailable>\naxis = 1 not in [-1, 1)\n\t [[{{node CollaGANAffluentGenerator/tf.unstack/unstack}}]] [Op:__inference_signature_wrapper_11625]\u001B[0m\n\nArguments received by MultiInputTFSMLayer.call():\n  • inputs=['tf.Tensor(shape=(16, 4, 64, 64, 4), dtype=float32)', 'tf.Tensor(shape=(16,), dtype=float32)']"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Running the Metrics Calculcation\n",
   "id": "27acb26e6828736b"
  },
  {
   "cell_type": "code",
   "id": "279c3c2cecdcea99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T18:55:55.580448Z",
     "start_time": "2025-02-12T18:55:42.123557Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "from itertools import combinations\n",
    "import evaluation_metrics\n",
    "\n",
    "DOMAINS = [\"back\", \"left\", \"front\", \"right\"]\n",
    "abbreviate_domain_list = lambda domains: \"+\".join(map(lambda d: d[0], domains))\n",
    "\n",
    "sanity_sample = dataset_loader.load_paired_images(240)\n",
    "\n",
    "def calculate_metrics_with_a_single_generator(model_name):\n",
    "    minimum_inputs = 1\n",
    "    maximum_inputs = len(DOMAINS) - 1\n",
    "    \n",
    "    metrics = {\n",
    "        target_domain_name: {\n",
    "            f\"Missing {maximum_inputs - available_inputs + 1}\": {\n",
    "                abbreviate_domain_list(combination): [None, None]\n",
    "                for combination in combinations(DOMAINS, available_inputs) if target_domain_name not in combination\n",
    "            } for available_inputs in range(minimum_inputs, maximum_inputs + 1)        \n",
    "        } for target_domain_name in DOMAINS\n",
    "    }\n",
    "    \n",
    "    selected_loader = model_loaders[model_name]\n",
    "    generator = selected_loader()\n",
    "    sanity_checked = False    \n",
    "    \n",
    "    # batch_size = 178    # for capacity-2, 178 works\n",
    "    batch_size = 64\n",
    "    logging.info(f\"Start >> Processing model {model_name}\")\n",
    "    for t, target_domain in enumerate(DOMAINS):\n",
    "        # determines the target images\n",
    "        logging.info(f\"Start >> Processing target {target_domain.upper()}\")\n",
    "        \n",
    "        # gets the metrics for the target images only once, to speed up\n",
    "        logging.debug(f\"Start >> Calculating FID partial metrics for target {target_domain.upper()}\")\n",
    "        real_images = np.stack(list(dataset_loader.dataset))\n",
    "        real_images = real_images[:, t]\n",
    "        real_images_metrics = evaluation_metrics.calculate_metrics_for_dataset(real_images, batch_size)\n",
    "        logging.debug(f\"End   >> Calculating FID partial metrics for target {target_domain.upper()}\")\n",
    "        \n",
    "        total_combinations_for_target = sum([sum(1 for ignore in combinations(DOMAINS, inputs)) for inputs in range(minimum_inputs, maximum_inputs + 1)]) // 2\n",
    "        combination_for_target_index = 0\n",
    "        get_progress = lambda: f\"{combination_for_target_index}/{total_combinations_for_target}\"\n",
    "        \n",
    "        for available_inputs in range(maximum_inputs, minimum_inputs - 1, -1):\n",
    "        #for available_inputs in [1]:\n",
    "            missing_inputs = maximum_inputs - available_inputs + 1\n",
    "            possible_input_combinations = list(combinations(DOMAINS, available_inputs))\n",
    "            logging.debug(f\"Start >> Processing {missing_inputs} missing input(s)\")\n",
    "            \n",
    "            for input_combination in possible_input_combinations:\n",
    "                abbreviated_source_domains = abbreviate_domain_list(input_combination)\n",
    "                logging.debug(f\"Start >> input combination: {abbreviated_source_domains}\")\n",
    "                if target_domain in input_combination:\n",
    "                    continue\n",
    "                \n",
    "                # determines the input images from the source domains\n",
    "                input_keep_mask = [1 if d in input_combination else 0 for d in DOMAINS]\n",
    "                logging.debug(f\"input_combination is {input_combination} and input_keep_mask is {input_keep_mask}\")\n",
    "                input_keep_mask = tf.constant(input_keep_mask, dtype=tf.float32)[tf.newaxis, :, tf.newaxis, tf.newaxis, tf.newaxis]\n",
    "                \n",
    "                # generates the images\n",
    "                logging.info(f\"Start >> Generating images {abbreviated_source_domains}-to-{target_domain} ({get_progress()})\")\n",
    "                # if not sanity_checked:\n",
    "                #     # sanity_checked = True\n",
    "                #     sanity_sample_dropped_out = sanity_sample * input_keep_mask\n",
    "                #     sanity_generated = generator.generate_from_multiple(tf.expand_dims(t, 0), sanity_sample_dropped_out)\n",
    "                #     fig = show_multiple_input_model_comparison(sanity_sample_dropped_out, tf.expand_dims(sanity_sample[t], 0), sanity_generated[tf.newaxis, ...], f\"Sanity check {abbreviated_source_domains}-to-{target_domain}\")\n",
    "                #     fig.savefig(f\"sample-outputs/{model_name},{abbreviated_source_domains}-to-{target_domain}.png\", transparent=True)\n",
    "                \n",
    "                fake_images = []\n",
    "                batched_dataset = dataset_loader.dataset.batch(batch_size)\n",
    "                for batch_number, batch in batched_dataset.enumerate():\n",
    "                    logging.debug(f\"Start >> Generating batch {batch_number}\")\n",
    "                    batch_transpose = tf.transpose(batch, [1, 0, 2, 3, 4])\n",
    "                    batch_transpose_dropped_out = batch_transpose * input_keep_mask\n",
    "                    target_input = tf.repeat(tf.constant([t]), len(batch_transpose))\n",
    "                    generated_images = generator.generate_from_multiple(target_input, batch_transpose_dropped_out)\n",
    "                    fake_images.append(generated_images.numpy())\n",
    "                    logging.debug(f\"End   >> Generating batch {batch_number}\")               \n",
    "\n",
    "                # calculates the metrics\n",
    "                fake_images = np.concatenate(fake_images, axis=0)\n",
    "                fake_images_metrics = evaluation_metrics.calculate_metrics_for_dataset(fake_images)\n",
    "                fid = evaluation_metrics.calculate_fid(real_images_metrics, fake_images_metrics)\n",
    "                l1 = evaluation_metrics.calculate_l1(real_images, fake_images)\n",
    "                metrics[target_domain][f\"Missing {missing_inputs}\"][abbreviated_source_domains] = [fid, l1.numpy()]\n",
    "                \n",
    "                combination_for_target_index += 1\n",
    "                logging.info(f\"End   >> Generating images {abbreviated_source_domains}-to-{target_domain} ({get_progress()})\")\n",
    "                    \n",
    "            logging.debug(f\"End   >> Processing {missing_inputs} missing input(s)\")\n",
    "            \n",
    "        logging.info(f\"End   >> Processing target {target_domain.upper()}\")\n",
    "    \n",
    "    logging.info(f\"End   >> Processing model {model_name}\")\n",
    "    del generator\n",
    "    return metrics\n",
    "\n",
    "resulting_metrics = {\n",
    "    # \"collagan,inpdrop-aggressive,rep-forward\": calculate_metrics_with_a_single_generator(\"collagan,inpdrop-aggressive,rep-forward\")\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "aeaf259393a31e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T13:49:32.043456Z",
     "start_time": "2025-01-28T13:49:32.020157Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "def generate_table_from_single_model(metrics):\n",
    "    df = pd.DataFrame(columns=[\"Missing\", \"Source\", \"Target\", \"FID\", \"L1\"])\n",
    "    for target_domain, target_metrics in metrics.items():\n",
    "        for missing_text, missing_metrics in target_metrics.items():\n",
    "            for abbreviated_source_domains, values in missing_metrics.items():\n",
    "                df = df.append({\n",
    "                    \"Missing\": missing_text,\n",
    "                    \"Source\": abbreviated_source_domains,\n",
    "                    \"Target\": target_domain,\n",
    "                    \"FID\": values[0] or \"\",\n",
    "                    \"L1\": values[1] or \"\"\n",
    "                }, ignore_index=True)\n",
    "                \n",
    "    return df\n",
    "            \n",
    "# generate_table_from_single_model(resulting_metrics[\"collagan,inpdrop-aggressive,rep-forward\"])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "1c8a5d3b3130205b",
   "metadata": {},
   "source": [
    "## Calculates the Metrics for All Models\n",
    "\n",
    "Calculate the metrics for all models.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "562ae4fcf916012a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:25:11.175054Z",
     "start_time": "2025-01-28T13:49:32.074547Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for model_name, model_loader in tqdm(model_loaders.items(), desc=\"Models\", total=len(model_loaders) - len(resulting_metrics)):\n",
    "    if model_name not in resulting_metrics:\n",
    "        resulting_metrics[model_name] = calculate_metrics_with_a_single_generator(model_name)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Models:   0%|          | 0/1 [00:00<?, ?it/s]D:\\Projetos 2\\dissertation-notebooks\\.venv\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:328: UserWarning: networks is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  config, custom_objects, \"function\", \"module\", \"function_type\"\n",
      "D:\\Projetos 2\\dissertation-notebooks\\.venv\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:328: UserWarning: networks is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  config, custom_objects, \"function\", \"module\", \"function_type\"\n",
      "D:\\Projetos 2\\dissertation-notebooks\\.venv\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:328: UserWarning: networks is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  config, custom_objects, \"function\", \"module\", \"function_type\"\n",
      "D:\\Projetos 2\\dissertation-notebooks\\.venv\\lib\\site-packages\\keras\\layers\\core\\lambda_layer.py:328: UserWarning: networks is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  config, custom_objects, \"function\", \"module\", \"function_type\"\n",
      "2025-01-28 10:49:50 INFO     Start >> Processing model ReMIC\n",
      "2025-01-28 10:49:50 INFO     Start >> Processing target BACK\n",
      "2025-01-28 10:49:54 INFO     Start >> Loading InceptionV3 model...\n",
      "2025-01-28 10:49:57 INFO     End   >> Loading InceptionV3 model.\n",
      "2025-01-28 10:50:27 INFO     Start >> Generating images l-to-back (0/7)\n",
      "2025-01-28 10:56:18 INFO     End   >> Generating images l-to-back (1/7)\n",
      "2025-01-28 10:56:18 INFO     Start >> Generating images f-to-back (1/7)\n",
      "2025-01-28 11:02:11 INFO     End   >> Generating images f-to-back (2/7)\n",
      "2025-01-28 11:02:11 INFO     Start >> Generating images r-to-back (2/7)\n",
      "2025-01-28 11:08:19 INFO     End   >> Generating images r-to-back (3/7)\n",
      "2025-01-28 11:08:19 INFO     End   >> Processing target BACK\n",
      "2025-01-28 11:08:19 INFO     Start >> Processing target LEFT\n",
      "2025-01-28 11:09:00 INFO     Start >> Generating images b-to-left (0/7)\n",
      "2025-01-28 11:15:35 INFO     End   >> Generating images b-to-left (1/7)\n",
      "2025-01-28 11:15:35 INFO     Start >> Generating images f-to-left (1/7)\n",
      "2025-01-28 11:22:30 INFO     End   >> Generating images f-to-left (2/7)\n",
      "2025-01-28 11:22:30 INFO     Start >> Generating images r-to-left (2/7)\n",
      "2025-01-28 11:30:02 INFO     End   >> Generating images r-to-left (3/7)\n",
      "2025-01-28 11:30:02 INFO     End   >> Processing target LEFT\n",
      "2025-01-28 11:30:02 INFO     Start >> Processing target FRONT\n",
      "2025-01-28 11:31:08 INFO     Start >> Generating images b-to-front (0/7)\n",
      "2025-01-28 11:39:21 INFO     End   >> Generating images b-to-front (1/7)\n",
      "2025-01-28 11:39:21 INFO     Start >> Generating images l-to-front (1/7)\n",
      "2025-01-28 11:47:38 INFO     End   >> Generating images l-to-front (2/7)\n",
      "2025-01-28 11:47:38 INFO     Start >> Generating images r-to-front (2/7)\n",
      "2025-01-28 11:55:59 INFO     End   >> Generating images r-to-front (3/7)\n",
      "2025-01-28 11:55:59 INFO     End   >> Processing target FRONT\n",
      "2025-01-28 11:55:59 INFO     Start >> Processing target RIGHT\n",
      "2025-01-28 11:57:29 INFO     Start >> Generating images b-to-right (0/7)\n",
      "2025-01-28 12:06:28 INFO     End   >> Generating images b-to-right (1/7)\n",
      "2025-01-28 12:06:28 INFO     Start >> Generating images l-to-right (1/7)\n",
      "2025-01-28 12:15:39 INFO     End   >> Generating images l-to-right (2/7)\n",
      "2025-01-28 12:15:39 INFO     Start >> Generating images f-to-right (2/7)\n",
      "2025-01-28 12:25:11 INFO     End   >> Generating images f-to-right (3/7)\n",
      "2025-01-28 12:25:11 INFO     End   >> Processing target RIGHT\n",
      "2025-01-28 12:25:11 INFO     End   >> Processing model ReMIC\n",
      "Models: 100%|██████████| 1/1 [1:35:38<00:00, 5738.97s/it]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "8014b430fd88fc35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T15:25:18.330063Z",
     "start_time": "2025-01-28T15:25:13.642049Z"
    }
   },
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_table_from_all_models(metrics):\n",
    "    model_dfs = [generate_table_from_single_model(m) for m in metrics.values()]\n",
    "    df = pd.concat(model_dfs, keys=[*metrics.keys()], names=[\"Model\"]).reset_index(level=[0])\n",
    "    return df\n",
    "\n",
    "today = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "metrics_df = generate_table_from_all_models(resulting_metrics)\n",
    "metrics_df.apply(pd.to_numeric, errors=\"ignore\")\n",
    "metrics_df.to_excel(f\"output/multiple-input-{today}.xlsx\", index=False)\n",
    "metrics_df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "    Model    Missing Source Target       FID        L1\n",
       "0   ReMIC  Missing 3      l   back  8.687722  0.059261\n",
       "1   ReMIC  Missing 3      f   back  5.654292  0.046083\n",
       "2   ReMIC  Missing 3      r   back  8.940358  0.059319\n",
       "3   ReMIC  Missing 2    l+f   back                    \n",
       "4   ReMIC  Missing 2    l+r   back                    \n",
       "5   ReMIC  Missing 2    f+r   back                    \n",
       "6   ReMIC  Missing 1  l+f+r   back                    \n",
       "7   ReMIC  Missing 3      b   left  5.148654  0.067098\n",
       "8   ReMIC  Missing 3      f   left  4.047487  0.059391\n",
       "9   ReMIC  Missing 3      r   left  1.226563  0.029602\n",
       "10  ReMIC  Missing 2    b+f   left                    \n",
       "11  ReMIC  Missing 2    b+r   left                    \n",
       "12  ReMIC  Missing 2    f+r   left                    \n",
       "13  ReMIC  Missing 1  b+f+r   left                    \n",
       "14  ReMIC  Missing 3      b  front  6.295907  0.055969\n",
       "15  ReMIC  Missing 3      l  front  8.131515  0.062532\n",
       "16  ReMIC  Missing 3      r  front  9.550296  0.063332\n",
       "17  ReMIC  Missing 2    b+l  front                    \n",
       "18  ReMIC  Missing 2    b+r  front                    \n",
       "19  ReMIC  Missing 2    l+r  front                    \n",
       "20  ReMIC  Missing 1  b+l+r  front                    \n",
       "21  ReMIC  Missing 3      b  right  5.795156  0.068104\n",
       "22  ReMIC  Missing 3      l  right  1.243322  0.029106\n",
       "23  ReMIC  Missing 3      f  right  3.963697  0.060212\n",
       "24  ReMIC  Missing 2    b+l  right                    \n",
       "25  ReMIC  Missing 2    b+f  right                    \n",
       "26  ReMIC  Missing 2    l+f  right                    \n",
       "27  ReMIC  Missing 1  b+l+f  right                    "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Missing</th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>FID</th>\n",
       "      <th>L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>l</td>\n",
       "      <td>back</td>\n",
       "      <td>8.687722</td>\n",
       "      <td>0.059261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>f</td>\n",
       "      <td>back</td>\n",
       "      <td>5.654292</td>\n",
       "      <td>0.046083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>r</td>\n",
       "      <td>back</td>\n",
       "      <td>8.940358</td>\n",
       "      <td>0.059319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>l+f</td>\n",
       "      <td>back</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>l+r</td>\n",
       "      <td>back</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>f+r</td>\n",
       "      <td>back</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 1</td>\n",
       "      <td>l+f+r</td>\n",
       "      <td>back</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>b</td>\n",
       "      <td>left</td>\n",
       "      <td>5.148654</td>\n",
       "      <td>0.067098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>f</td>\n",
       "      <td>left</td>\n",
       "      <td>4.047487</td>\n",
       "      <td>0.059391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>r</td>\n",
       "      <td>left</td>\n",
       "      <td>1.226563</td>\n",
       "      <td>0.029602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>b+f</td>\n",
       "      <td>left</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>b+r</td>\n",
       "      <td>left</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>f+r</td>\n",
       "      <td>left</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 1</td>\n",
       "      <td>b+f+r</td>\n",
       "      <td>left</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>b</td>\n",
       "      <td>front</td>\n",
       "      <td>6.295907</td>\n",
       "      <td>0.055969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>l</td>\n",
       "      <td>front</td>\n",
       "      <td>8.131515</td>\n",
       "      <td>0.062532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>r</td>\n",
       "      <td>front</td>\n",
       "      <td>9.550296</td>\n",
       "      <td>0.063332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>b+l</td>\n",
       "      <td>front</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>b+r</td>\n",
       "      <td>front</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>l+r</td>\n",
       "      <td>front</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 1</td>\n",
       "      <td>b+l+r</td>\n",
       "      <td>front</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>b</td>\n",
       "      <td>right</td>\n",
       "      <td>5.795156</td>\n",
       "      <td>0.068104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>l</td>\n",
       "      <td>right</td>\n",
       "      <td>1.243322</td>\n",
       "      <td>0.029106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 3</td>\n",
       "      <td>f</td>\n",
       "      <td>right</td>\n",
       "      <td>3.963697</td>\n",
       "      <td>0.060212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>b+l</td>\n",
       "      <td>right</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>b+f</td>\n",
       "      <td>right</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 2</td>\n",
       "      <td>l+f</td>\n",
       "      <td>right</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ReMIC</td>\n",
       "      <td>Missing 1</td>\n",
       "      <td>b+l+f</td>\n",
       "      <td>right</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
